Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.




  0%|                                                                           | 0/4000 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.











  0%|▏                                                               | 14/4000 [00:37<1:59:00,  1.79s/it]Traceback (most recent call last):
  File "/data/2_data_server/nlp-04/lost_technology/test.py", line 113, in <module>
    trainer.train()
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/trainer.py", line 1591, in train
    return inner_training_loop(
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/trainer.py", line 1971, in _inner_training_loop
    self.optimizer.step()
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/optim/adamw.py", line 162, in step
    adamw(params_with_grad,
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/optim/adamw.py", line 219, in adamw
    func(params,
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/optim/adamw.py", line 274, in _single_tensor_adamw
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt