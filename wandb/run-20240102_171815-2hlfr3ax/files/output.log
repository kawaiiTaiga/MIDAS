Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

  0%|                                                                                       | 0/4000 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.











































































































 10%|███████▋                                                                     | 400/4000 [03:47<33:03,  1.81it/s]


 68%|██████████████████████████████████████████████████████▍                         | 17/25 [00:03<00:01,  4.61it/s]


























 12%|█████████▌                                                                   | 497/4000 [04:44<31:21,  1.86it/s]


















































































 20%|███████████████▍                                                             | 800/4000 [07:29<28:36,  1.86it/s]


 96%|████████████████████████████████████████████████████████████████████████████▊   | 24/25 [00:05<00:00,  4.63it/s]























































 25%|███████████████████                                                         | 1001/4000 [09:24<26:32,  1.88it/s]





















































 30%|██████████████████████▊                                                     | 1200/4000 [11:12<25:45,  1.81it/s]


 88%|██████████████████████████████████████████████████████████████████████▍         | 22/25 [00:04<00:00,  4.60it/s]

















































































 37%|████████████████████████████▍                                               | 1498/4000 [13:59<22:09,  1.88it/s]



























 40%|██████████████████████████████▍                                             | 1600/4000 [14:54<21:54,  1.83it/s]


 88%|██████████████████████████████████████████████████████████████████████▍         | 22/25 [00:04<00:00,  4.09it/s]












































































































 50%|█████████████████████████████████████▉                                      | 1998/4000 [18:35<18:31,  1.80it/s]
 50%|██████████████████████████████████████                                      | 2000/4000 [18:36<17:54,  1.86it/s]


 88%|██████████████████████████████████████████████████████████████████████▍         | 22/25 [00:04<00:00,  4.39it/s]












































































































 60%|█████████████████████████████████████████████▌                              | 2400/4000 [22:17<15:12,  1.75it/s]


 68%|██████████████████████████████████████████████████████▍                         | 17/25 [00:03<00:01,  4.17it/s]



























 62%|███████████████████████████████████████████████▌                            | 2500/4000 [23:17<13:05,  1.91it/s]

















































































 70%|█████████████████████████████████████████████████████▏                      | 2800/4000 [26:00<12:07,  1.65it/s]


 88%|██████████████████████████████████████████████████████████████████████▍         | 22/25 [00:04<00:00,  4.46it/s]























































 75%|█████████████████████████████████████████████████████████                   | 3002/4000 [27:56<08:40,  1.92it/s]





















































 80%|████████████████████████████████████████████████████████████▊               | 3200/4000 [29:43<07:34,  1.76it/s]


 88%|██████████████████████████████████████████████████████████████████████▍         | 22/25 [00:04<00:00,  4.67it/s]





 80%|█████████████████████████████████████████████████████████████               | 3217/4000 [29:58<07:14,  1.80it/s]Traceback (most recent call last):
  File "/data/2_data_server/nlp-04/lost_technology/test.py", line 113, in <module>
    trainer.train()
  File "/home/nlp-04/anaconda3/envs/heck/lib/python3.11/site-packages/transformers/trainer.py", line 1555, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/nlp-04/anaconda3/envs/heck/lib/python3.11/site-packages/transformers/trainer.py", line 1842, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt