Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



Map:  86%|███████████████████████████████████████████████████▊        | 34000/39389 [00:05<00:00, 6678.29 examples/s]
  0%|                                                                                       | 0/4000 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.































































































































































































  5%|████                                                                       | 217/4000 [06:39<1:53:21,  1.80s/it]Traceback (most recent call last):
  File "/data/2_data_server/nlp-04/lost_technology/test.py", line 110, in <module>
    trainer.train()
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/trainer.py", line 1591, in train
    return inner_training_loop(
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/trainer.py", line 1892, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/trainer.py", line 2776, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/trainer.py", line 2801, in compute_loss
    outputs = model(**inputs)
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 81, in parallel_apply
    thread.join()
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt