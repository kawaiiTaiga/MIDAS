Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

  0%|                                                                                       | 0/4000 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.



































































































 10%|███████▋                                                                     | 400/4000 [03:30<31:07,  1.93it/s]


 64%|███████████████████████████████████████████████████▏                            | 16/25 [00:03<00:02,  4.39it/s]
























 12%|█████████▌                                                                   | 497/4000 [04:24<28:32,  2.05it/s]











































































 20%|███████████████▍                                                             | 800/4000 [06:54<26:54,  1.98it/s]


 64%|███████████████████████████████████████████████████▏                            | 16/25 [00:03<00:02,  4.08it/s]


















































 25%|███████████████████▏                                                         | 999/4000 [08:40<25:11,  1.99it/s]

















































 30%|██████████████████████▊                                                     | 1200/4000 [10:19<23:43,  1.97it/s]


 88%|██████████████████████████████████████████████████████████████████████▍         | 22/25 [00:04<00:00,  4.65it/s]












































































 38%|████████████████████████████▌                                               | 1500/4000 [12:56<21:00,  1.98it/s]

























 40%|██████████████████████████████▍                                             | 1600/4000 [13:47<20:20,  1.97it/s]


 96%|████████████████████████████████████████████████████████████████████████████▊   | 24/25 [00:05<00:00,  4.64it/s]



































































































 50%|█████████████████████████████████████▉                                      | 1999/4000 [17:11<15:59,  2.09it/s]
 50%|██████████████████████████████████████                                      | 2000/4000 [17:11<15:28,  2.15it/s]


 92%|█████████████████████████████████████████████████████████████████████████▌      | 23/25 [00:04<00:00,  4.65it/s]



































































































 60%|█████████████████████████████████████████████▌                              | 2400/4000 [20:36<12:35,  2.12it/s]


 88%|██████████████████████████████████████████████████████████████████████▍         | 22/25 [00:04<00:00,  4.08it/s]

























 62%|███████████████████████████████████████████████▍                            | 2499/4000 [21:30<12:26,  2.01it/s]











































































 70%|█████████████████████████████████████████████████████▏                      | 2800/4000 [24:02<09:35,  2.09it/s]


 88%|██████████████████████████████████████████████████████████████████████▍         | 22/25 [00:04<00:00,  4.44it/s]




 70%|█████████████████████████████████████████████████████▍                      | 2814/4000 [24:15<10:17,  1.92it/s]Traceback (most recent call last):
  File "/data/2_data_server/nlp-04/lost_technology/test.py", line 113, in <module>
    trainer.train()
  File "/home/nlp-04/anaconda3/envs/heck/lib/python3.11/site-packages/transformers/trainer.py", line 1555, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/nlp-04/anaconda3/envs/heck/lib/python3.11/site-packages/transformers/trainer.py", line 1842, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt